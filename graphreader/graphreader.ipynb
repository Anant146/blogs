{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomasonjo/blogs/blob/master/llm/graphreader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Az8JvO0tmR9h",
    "outputId": "f36037f7-b6e8-4e0b-cefa-50b54bf2229b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet neo4j langchain-community langchain-core langchain-openai langchain-text-splitters tiktoken wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QgQQ5YRFoc2O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kShKiVoDrLjk",
    "outputId": "ac08993b-3a35-4047-f4ca-c609a7926736"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Neo4jGraph(refresh_schema=False)\n",
    "\n",
    "graph.query(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\"\"\")\n",
    "graph.query(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE\"\"\")\n",
    "graph.query(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QpCb9N75pSb1"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(doc_content_chars_max=10000))\n",
    "\n",
    "text = wikipedia.run(\"Joan of Arc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6581"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M8ZpVz7XmaMz"
   },
   "outputs": [],
   "source": [
    "construction_system = \"\"\"\n",
    "You are now an intelligent assistant tasked with meticulously extracting both key elements and\n",
    "atomic facts from a long text.\n",
    "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\n",
    "actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
    "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include\n",
    "propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
    "sequences, interpersonal relationships, timelines, etc.\n",
    "Requirements:\n",
    "#####\n",
    "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
    "2. You should extract key elements and atomic facts comprehensively, especially those that are\n",
    "important and potentially query-worthy and do not leave out details.\n",
    "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He,\n",
    "She to actual names).\n",
    "4. Ensure that the key elements and atomic facts you extract are presented in the same language as\n",
    "the original text (e.g., English or Chinese).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lV3DgZfvpjk1"
   },
   "outputs": [],
   "source": [
    "construction_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            construction_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Use the given format to extract information from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "nG3PAV8xnc9Z"
   },
   "outputs": [],
   "source": [
    "class AtomicFact(BaseModel):\n",
    "    key_elements: List[str] = Field(description=\"\"\"The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\n",
    "actions), and adjectives (e.g., states, feelings) that are pivotal to the atomic fact's narrative.\"\"\")\n",
    "    atomic_fact: str = Field(description=\"\"\"The smallest, indivisible facts, presented as concise sentences. These include\n",
    "propositions, theories, existences, concepts, and implicit elements like logic, causality, event\n",
    "sequences, interpersonal relationships, timelines, etc.\"\"\")\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    atomic_facts: List[AtomicFact] = Field(description=\"List of atomic facts\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.3)\n",
    "structured_llm = model.with_structured_output(Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "wqz316TGp4C2"
   },
   "outputs": [],
   "source": [
    "construction_chain = construction_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8XzROIKp7hL",
    "outputId": "828909ab-8035-45a9-e06a-6fda9de7d452"
   },
   "outputs": [],
   "source": [
    "result = construction_chain.invoke({\"input\":text})\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "_1oTe3--1PFu"
   },
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (c:Chunk {id: row.chunk_id})\n",
    "SET c.text = row.chunk_text,\n",
    "    c.index = row.index,\n",
    "    c.document_name = row.document_name\n",
    "WITH c, row\n",
    "UNWIND row.atomic_facts AS af\n",
    "MERGE (a:AtomicFact {id: af.id})\n",
    "SET a.text = af.atomic_fact\n",
    "MERGE (c)-[:HAS_ATOMIC_FACT]->(a)\n",
    "WITH c, a, af\n",
    "UNWIND af.key_elements AS ke\n",
    "MERGE (k:KeyElement {id: ke})\n",
    "MERGE (a)-[:HAS_KEY_ELEMENT]->(k)\n",
    "\"\"\"\n",
    "\n",
    "def encode_md5(text):\n",
    "    return md5(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper used 2k token size\n",
    "async def process_document(text, document_name, chunk_size=2000, chunk_overlap=200):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    print(f\"Total text chunks: {len(texts)}\")\n",
    "    tasks = [\n",
    "        asyncio.create_task(construction_chain.ainvoke({\"input\":chunk_text}))\n",
    "        for index, chunk_text in enumerate(texts)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    docs = [el.dict() for el in results]\n",
    "    for index, doc in enumerate(docs):\n",
    "        doc['chunk_id'] = encode_md5(texts[index])\n",
    "        doc['chunk_text'] = texts[index]\n",
    "        doc['index'] = index\n",
    "        for af in doc[\"atomic_facts\"]:\n",
    "            af[\"id\"] = encode_md5(af[\"atomic_fact\"])\n",
    "    graph.query(import_query, \n",
    "            params={\"data\": docs})\n",
    "    graph.query(\"\"\"MATCH (c:Chunk) WHERE c.document_name = $document_name\n",
    "WITH c ORDER BY c.index WITH collect(c) AS nodes CALL apoc.nodes.link(nodes, 'NEXT')\"\"\",\n",
    "           params={\"document_name\":document_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "jDiIcy9H3njG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text chunks: 4\n"
     ]
    }
   ],
   "source": [
    "await process_document(text, \"Joan of Arc\", chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__7YY7jS3pKc"
   },
   "source": [
    "# Agent part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rational_plan_system = \"\"\"As an intelligent assistant, your primary objective is to answer the question by gathering\n",
    "supporting facts from a given article. To facilitate this objective, the first step is to make\n",
    "a rational plan based on the question. This plan should outline the step-by-step process to\n",
    "resolve the question and specify the key information required to formulate a comprehensive answer.\n",
    "Example:\n",
    "#####\n",
    "User: Who had a longer tennis career, Danny or Alice?\n",
    "Assistant: In order to answer this question, we first need to find the length of Danny’s\n",
    "and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the\n",
    "two.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin.\"\"\"\n",
    "\n",
    "rational_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            rational_plan_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"{input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rational_chain = rational_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_node_system = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
    "comprising the following elements:\n",
    "1. Text Chunks: Chunks of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the\n",
    "rational plan, and a list of node key elements. These initial nodes are crucial because they are the\n",
    "starting point for searching for relevant information.\n",
    "Requirements:\n",
    "#####\n",
    "1. Once you have selected a starting node, assess its relevance to the potential answer by assigning\n",
    "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
    "whereas a score of 0 suggests minimal relevance.\n",
    "2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format\n",
    "each line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n",
    "3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n",
    "4. In the user’s input, each line constitutes a node. When selecting the starting node, please make\n",
    "your choice from those provided, and refrain from fabricating your own. The nodes you output\n",
    "must correspond exactly to the nodes given by the user, with identical wording.\n",
    "Finally, I emphasize again that you need to select the starting node from the given Nodes, and\n",
    "it must be consistent with the words of the node you selected. Please strictly follow the above\n",
    "format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "initial_node_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            initial_node_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Nodes: {nodes}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Node(BaseModel):\n",
    "    key_element: str = Field(description=\"\"\"Key element or name of a relevant node\"\"\")\n",
    "    score: int = Field(description=\"\"\"Relevance to the potential answer by assigning\n",
    "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
    "whereas a score of 0 suggests minimal relevance.\"\"\")\n",
    "\n",
    "class InitialNodes(BaseModel):\n",
    "    initial_nodes: List[Node] = Field(description=\"List of relevant nodes to the question and plan\")\n",
    "\n",
    "initial_nodes_chain = initial_node_prompt | model.with_structured_output(InitialNodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 72, offset: 71} for query: \"UNWIND $data AS row MATCH (n:`KeyElement`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "source": [
    "# Just sending all nodes sounds like a not scalable solution, \n",
    "# so we will use vector index to retrieve top 50 most relevant nodes\n",
    "\n",
    "nodes_vector = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    index_name=\"keyelements\",\n",
    "    node_label=\"KeyElement\",\n",
    "    text_node_properties=[\"id\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    retrieval_query=\"RETURN node.id AS text, score, {} AS metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_nodes(question: str) -> List[str]:\n",
    "    data = nodes_vector.similarity_search(question, k=50)\n",
    "    return [el.page_content for el in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joan of Arc', 'The Passion of Joan of Arc', 'siege of Orléans', 'Loire Campaign', 'Hundred Years War', \"Hundred Years' War\", 'Joan', 'French army', 'Charles VII', 'King of France', 'Burgundian troops', 'French morale', 'French nationalists', 'burned at the stake', 'Archbishop of Paris', 'Saint Margaret', 'Burgundians', 'French Revolution', 'Saint Catherine', 'besieged', 'siege', 'northeast France', 'French', 'French nation', 'Bishop Pierre Cauchon', 'English domination', 'Compiègne', 'military leader', 'April 1429', 'Rouen prison', 'France', \"court's faith\", 'canonized', 'early feminist', 'martyr', 'Renée Jeanne Falconetti', 'Paris', 'patron saint', 'medieval architecture', 'Faye Dunaway', '30 May 1431', 'iègne', 'historical icons', 'inquisitorial court', 'Milla Jovovich', 'unsuccessful', 'Roman Catholic Church', 'La Charité', 'nineteen', 'silent historical film']\n"
     ]
    }
   ],
   "source": [
    "nodes = get_potential_nodes(question)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(key_element='Joan of Arc', score=100), Node(key_element='military leader', score=95), Node(key_element='siege of Orléans', score=90), Node(key_element='Loire Campaign', score=85), Node(key_element='unsuccessful', score=85)]\n"
     ]
    }
   ],
   "source": [
    "initial_nodes = initial_nodes_chain.invoke({\"question\":question, \"rational_plan\":rational_plan, \"nodes\":nodes})\n",
    "# paper uses 5 initial nodes\n",
    "atomic_facts_check_queue = sorted(initial_nodes.initial_nodes, key=lambda node: node.score, reverse=True)[:5]\n",
    "print(atomic_facts_check_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atomic_facts(key_elements: List[str]) -> List[Dict[str, str]]:\n",
    "    data = graph.query(\"\"\"\n",
    "    MATCH (k:KeyElement)<-[:HAS_KEY_ELEMENT]-(fact)<-[:HAS_ATOMIC_FACT]-(chunk)\n",
    "    WHERE k.id IN $key_elements\n",
    "    RETURN distinct chunk.id AS chunk_id, fact.text AS text\n",
    "    \"\"\", params={\"key_elements\": key_elements})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_fact_check_system = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
    "comprising the following elements:\n",
    "1. Text Chunks: Chunks of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to check a node and its associated atomic facts, with the objective of\n",
    "determining whether to proceed with reviewing the text chunk corresponding to these atomic facts.\n",
    "Given the question, the rational plan, previous actions, notebook content, and the current node’s\n",
    "atomic facts and their corresponding chunk IDs, you have the following Action Options:\n",
    "#####\n",
    "1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
    "fact may hold the necessary information to answer the question. This will allow you to access\n",
    "more complete and detailed information.\n",
    "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
    "information.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting nodes or chunks.\n",
    "2. You can choose to read multiple text chunks at the same time.\n",
    "3. Atomic facts only cover part of the information in the text chunk, so even if you feel that the\n",
    "atomic facts are slightly relevant to the question, please try to read the text chunk to get more\n",
    "complete information.\n",
    "#####\n",
    "Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the\n",
    "question, you should still look at the text chunk to avoid missing information. You should only\n",
    "choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to\n",
    "the question. Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "class AtomicFactOutput(BaseModel):\n",
    "    updated_notebook: str = Field(description=\"\"\"First, combine your current notebook with new insights and findings about\n",
    "the question from current atomic facts, creating a more complete version of the notebook that\n",
    "contains more valid information.\"\"\")\n",
    "    rational_next_action: str = Field(description=\"\"\"Based on the given question, the rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
    "fact may hold the necessary information to answer the question. This will allow you to access\n",
    "more complete and detailed information.\n",
    "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
    "information.\"\"\")\n",
    "\n",
    "atomic_fact_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            atomic_fact_check_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Atomic facts: {atomic_facts}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "atomic_fact_chain = atomic_fact_check_prompt | model.with_structured_output(AtomicFactOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_function(input_str):\n",
    "    # Regular expression to capture the function name and arguments\n",
    "    pattern = r'(\\w+)(?:\\((.*)\\))?'\n",
    "    \n",
    "    match = re.match(pattern, input_str)\n",
    "    if match:\n",
    "        function_name = match.group(1)  # Extract the function name\n",
    "        raw_arguments = match.group(2)  # Extract the arguments as a string        \n",
    "        # If there are arguments, attempt to parse them\n",
    "        arguments = []\n",
    "        if raw_arguments:\n",
    "            try:\n",
    "                # Use ast.literal_eval to safely evaluate and convert the arguments\n",
    "                parsed_args = ast.literal_eval(f'({raw_arguments})')  # Wrap in tuple parentheses\n",
    "                # Ensure it's always treated as a tuple even with a single argument\n",
    "                arguments = list(parsed_args) if isinstance(parsed_args, tuple) else [parsed_args]\n",
    "            except (ValueError, SyntaxError):\n",
    "                # In case of failure to parse, return the raw argument string\n",
    "                arguments = [raw_arguments.strip()]\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'function_name': function_name,\n",
    "            'arguments': arguments\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(chunk_ids: List[str]) -> List[Dict[str, str]]:\n",
    "    data = graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.id IN $chunk_ids\n",
    "    RETURN distinct c.id AS chunk_id, c.text AS text\n",
    "    \"\"\", params={\"chunk_ids\": chunk_ids})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(nodes):\n",
    "    graph_query(\"\"\"\n",
    "    MATCH (k:KeyElement)<-[:HAS_KEY_ELEMENT]-()-[:HAS_KEY_ELEMENT]->(neighbor)\n",
    "    WHERE k.id IN $nodes AND NOT neighbor.id IN $nodes\n",
    "    RETURN neighbor.id AS key_element, count(*) AS count\n",
    "    ORDER BY count DESC LIMIT 25\n",
    "    \"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_read_system_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to assess a specific text chunk and determine whether the available information\n",
    "suffices to answer the question. Given the question, rational plan, previous actions, notebook\n",
    "content, and the current text chunk, you have the following Action Options:\n",
    "#####\n",
    "1. search_more(): Choose this action if you think that the essential information necessary to\n",
    "answer the question is still lacking.\n",
    "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
    "valuable information for answering the question.\n",
    "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
    "valuable information for answering the question.\n",
    "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
    "is enough to answer the question. This will allow you to summarize the gathered information and\n",
    "provide a final answer.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
    "2. You can only choose one action.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin\n",
    "\"\"\"\n",
    "\n",
    "class ChunkOutput(BaseModel):\n",
    "    updated_notebook: str = Field(description=\"\"\"First, combine your previous notes with new insights and findings about the\n",
    "question from current text chunks, creating a more complete version of the notebook that contains\n",
    "more valid information.\"\"\")\n",
    "    rational_next_move: str = Field(description=\"\"\"Based on the given question, rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"1. search_more(): Choose this action if you think that the essential information necessary to\n",
    "answer the question is still lacking.\n",
    "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
    "valuable information for answering the question.\n",
    "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
    "valuable information for answering the question.\n",
    "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
    "is enough to answer the question. This will allow you to summarize the gathered information and\n",
    "provide a final answer.\"\"\")\n",
    "\n",
    "chunk_read_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            chunk_read_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Chunk: {chunk}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chunk_read_chain = chunk_read_prompt | model.with_structured_output(ChunkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_select_system_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to assess all neighboring nodes of the current node, with the objective of determining whether to proceed to the next neighboring node. Given the question, rational\n",
    "plan, previous actions, notebook content, and the neighbors of the current node, you have the\n",
    "following Action Options:\n",
    "#####\n",
    "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
    "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
    "on one neighbor node at a time.\n",
    "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
    "information that could answer the question.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
    "2. You can only choose one action. This means that you can choose to read only one neighbor\n",
    "node or choose to terminate.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "class NeighborOutput(BaseModel):\n",
    "    rational_next_action: str = Field(description=\"\"\"Based on the given question, rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"You have the following Action Options:\n",
    "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
    "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
    "on one neighbor node at a time.\n",
    "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
    "information that could answer the question.\"\"\")\n",
    "\n",
    "neighbor_select_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            neighbor_select_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Neighbor nodes: {nodes}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "neighbor_select_chain = neighbor_select_prompt | model.with_structured_output(NeighborOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequent_chunk_id(chunk):\n",
    "    data = graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)-[:NEXT]->(next)\n",
    "    WHERE c.id = $id\n",
    "    RETURN next.id AS next\n",
    "    \"\"\")\n",
    "    return data\n",
    "\n",
    "def get_previous_chunk_id(chunk):\n",
    "    data = graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)<-[:NEXT]-(previous)\n",
    "    WHERE c.id = $id\n",
    "    RETURN previous.id AS previous\n",
    "    \"\"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rational plan is:In order to answer this question, we first need to identify the key battles that Joan of Arc participated in during her military career. Then, we need to determine the outcomes of these battles to see if there were any that she lost.\n",
      "Initial node selection:['Joan of Arc', 'unsuccessful', 'siege of Orléans', 'Loire Campaign', 'Compiègne']\n",
      "Reading atomic facts about: ['Joan of Arc', 'unsuccessful', 'siege of Orléans', 'Loire Campaign', 'Compiègne']\n",
      "Rational for next action after atomic check: To answer the question about whether Joan of Arc lost any battles, the provided atomic facts indicate that she did experience defeats in some battles, such as the unsuccessful sieges of Paris and La Charité. Therefore, further exploration of these specific battles could provide more detailed information about the circumstances and outcomes of these defeats.\n",
      "Function signature: {'function_name': 'read_chunk', 'arguments': [['3feaa3f3137a5f839bfc207bc5c2ffdb']]}\n",
      "updated_notebook='Joan of Arc participated in several key battles during her military career, including the siege of Orléans and the Loire Campaign. She experienced victories in these battles, such as lifting the siege of Orléans and winning at Patay. However, she also faced defeats, including the unsuccessful sieges of Paris and La Charité.' rational_next_action='To answer the question about whether Joan of Arc lost any battles, the provided atomic facts indicate that she did experience defeats in some battles, such as the unsuccessful sieges of Paris and La Charité. Therefore, further exploration of these specific battles could provide more detailed information about the circumstances and outcomes of these defeats.' chosen_action=\"read_chunk(['3feaa3f3137a5f839bfc207bc5c2ffdb'])\"\n",
      "Reading chunk id: 3feaa3f3137a5f839bfc207bc5c2ffdb\n",
      "updated_notebook='Joan of Arc participated in several key battles during her military career, including the siege of Orléans and the Loire Campaign. She experienced victories in these battles, such as lifting the siege of Orléans and winning at Patay. However, she also faced defeats, including the unsuccessful sieges of Paris and La Charité.' rational_next_move='The current text chunk confirms the defeats Joan of Arc faced, specifically mentioning the unsuccessful sieges of Paris and La Charité. This information aligns with the notebook content and provides the necessary details to answer the question about whether Joan of Arc lost any battles.' chosen_action='termination()'\n",
      "Rational for next action after reading chunks: The current text chunk confirms the defeats Joan of Arc faced, specifically mentioning the unsuccessful sieges of Paris and La Charité. This information aligns with the notebook content and provides the necessary details to answer the question about whether Joan of Arc lost any battles.\n",
      "Function signature: {'function_name': 'termination', 'arguments': []}\n"
     ]
    }
   ],
   "source": [
    "question = \"Did Joan of Arc lose any battles?\"\n",
    "\n",
    "notebook = \"\"\n",
    "previous_actions = []\n",
    "chunks_check_queue = []\n",
    "\n",
    "rational_plan = rational_chain.invoke({\"input\": question})\n",
    "print(f\"Rational plan is:{rational_plan}\")\n",
    "initial_nodes = initial_nodes_chain.invoke({\"question\":question, \"rational_plan\":rational_plan, \"nodes\":nodes})\n",
    "# paper uses 5 initial nodes\n",
    "atomic_facts_check_queue = [el.key_element for el in sorted(initial_nodes.initial_nodes, key=lambda node: node.score, reverse=True)][:5]\n",
    "print(f\"Initial node selection:{atomic_facts_check_queue}\")\n",
    "while True:\n",
    "    if atomic_facts_check_queue:\n",
    "        # Atomic facts are processed in a single LLM call\n",
    "        atomic_facts = get_atomic_facts(atomic_facts_check_queue)\n",
    "        print(f\"Reading atomic facts about: {atomic_facts_check_queue}\")\n",
    "        atomic_facts_results = atomic_fact_chain.invoke({\"question\":question, \"rational_plan\": rational_plan, \n",
    "                                    \"notebook\": notebook, \"atomic_facts\": atomic_facts,\n",
    "                                   \"previous_actions\": previous_actions})\n",
    "        # Reset atomic facts queue\n",
    "        atomic_facts_check_queue = []\n",
    "        notebook = atomic_facts_results.updated_notebook\n",
    "        print(f\"Rational for next action after atomic check: {atomic_facts_results.rational_next_action}\")\n",
    "        chosen_action = parse_function(atomic_facts_results.chosen_action)\n",
    "        print(f\"Function signature: {chosen_action}\")\n",
    "        if chosen_action.get(\"function_name\") == \"stop_and_read_neighbor\":\n",
    "            neighbors = get_neighbors([el.key_element for el in atomic_facts_queue])\n",
    "            # Pass neighbors to chain\n",
    "            read_neighbor_results = neighbor_select_chain.invoke({\n",
    "                {\"question\":question, \"rational_plan\": rational_plan, \n",
    "                                                \"notebook\": notebook, \"nodes\": nodes,\n",
    "                                               \"previous_actions\": previous_actions}\n",
    "            })\n",
    "            # Reset atomic facts queue\n",
    "            print(f\"Rational for next action after reading neighbors: {read_neighbor_results.rational_next_move}\")\n",
    "            chosen_action = parse_function(read_neighbor_results.chosen_action)\n",
    "            print(f\"Function signature: {chosen_action}\")\n",
    "            if chosen_action.get(\"function_name\") == 'termination':\n",
    "                break\n",
    "            elif chosen_action.get(\"function_name\") == 'read_neighbor_node':\n",
    "                atomic_facts_check_queue = [chosen_action.get(\"arguments\")[0]]\n",
    "                continue\n",
    "                \n",
    "        elif chosen_action.get(\"function_name\") == \"read_chunk\":\n",
    "            chunks_check_queue = chosen_action.get(\"arguments\")[0]\n",
    "            continue\n",
    "    if chunks_check_queue:\n",
    "        # Get the first chunk\n",
    "        chunk_id = chunks_check_queue.pop()\n",
    "        print(f\"Reading chunk id: {chunk_id}\")\n",
    "        chunks_text = get_chunks([chunk_id])\n",
    "        read_chunk_results = chunk_read_chain.invoke({\"question\":question, \"rational_plan\": rational_plan, \n",
    "                                    \"notebook\": notebook, \"chunk\": chunks_text,\n",
    "                                   \"previous_actions\": previous_actions})\n",
    "        print(read_chunk_results)\n",
    "        notebook = results.updated_notebook\n",
    "        print(f\"Rational for next action after reading chunks: {read_chunk_results.rational_next_move}\")\n",
    "        chosen_action = parse_function(read_chunk_results.chosen_action)\n",
    "        print(f\"Function signature: {chosen_action}\")\n",
    "        if chosen_action.get('function_name') == 'read_subsequent_chunk':\n",
    "            subsequent_id = get_subsequent_chunk_id(chunk_id)\n",
    "            chunks_check_queue.append(subsequent_id)\n",
    "            continue\n",
    "            \n",
    "        elif chosen_action.get('function_name') == 'read_previous_chunk':\n",
    "            previous_id = get_previous_chunk_id(chunk_id)\n",
    "            chunks_check_queue.append(previous_id)\n",
    "            continue\n",
    "            \n",
    "        elif chosen_action.get('function_name') == 'search_more':\n",
    "            # Go over to next chunk\n",
    "            if chunks_check_queue:\n",
    "                continue\n",
    "            else:\n",
    "                # Get neighbors: Todo\n",
    "                continue\n",
    "        elif chosen_action.get('function_name') == 'termination':\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joan of Arc participated in several battles during her military career. She played a significant role in the siege of Orléans, which was a victory for the French. She also encouraged the French to pursue the English during the Loire Campaign, leading to another victory at Patay. However, Joan of Arc also experienced defeats, such as the unsuccessful siege of Paris in September 1429 and the failed siege of La Charité in November. These defeats reduced the court's faith in her. Additionally, in early 1430, she organized a company of volunteers to relieve Compiègne, but she was captured by Burgundian troops in May 1430.\n"
     ]
    }
   ],
   "source": [
    "print(notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_reasoning_system_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "You have now explored multiple paths from various starting nodes on this graph, recording key information for each path in a notebook.\n",
    "Your task now is to analyze these memories and reason to answer the question.\n",
    "Strategy:\n",
    "#####\n",
    "1. You should first analyze each notebook content before providing a final answer.\n",
    "2. During the analysis, consider complementary information from other notes and employ a\n",
    "majority voting strategy to resolve any inconsistencies.\n",
    "3. When generating the final answer, ensure that you take into account all available information.\n",
    "#####\n",
    "Example:\n",
    "#####\n",
    "User:\n",
    "Question: Who had a longer tennis career, Danny or Alice?\n",
    "Notebook of different exploration paths:\n",
    "1. We only know that Danny’s tennis career started in 1972 and ended in 1990, but we don’t know\n",
    "the length of Alice’s career.\n",
    "2. ......\n",
    "Assistant:\n",
    "Analyze:\n",
    "The summary of search path 1 points out that Danny’s tennis career is 1990-1972=18 years.\n",
    "Although it does not indicate the length of Alice’s career, the summary of search path 2 finds this\n",
    "information, that is, the length of Alice’s tennis career is 15 years. Then we can get the final\n",
    "answer, that is, Danny’s tennis career is longer than Alice’s.\n",
    "Final answer:\n",
    "Danny’s tennis career is longer than Alice’s.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin\n",
    "\"\"\"\n",
    "\n",
    "answer_reasoning_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            answer_reasoning_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Notebook: {notebook}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class AnswerReasonOutput(BaseModel):\n",
    "    analyze: str = Field(description=\"\"\"You should first analyze each notebook content before providing a final answer.\n",
    "    During the analysis, consider complementary information from other notes and employ a\n",
    "majority voting strategy to resolve any inconsistencies.\"\"\")\n",
    "    final_answer: str = Field(description=\"\"\"When generating the final answer, ensure that you take into account all available information.\"\"\")\n",
    "\n",
    "answer_reasoning_chain = answer_reasoning_prompt | model.with_structured_output(AnswerReasonOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerReasonOutput(analyze='The notebook indicates that Joan of Arc participated in multiple battles, achieving victories in some, like the siege of Orléans and the battle of Patay. However, she also faced defeats, specifically during the siege of Paris in September 1429 and the siege of La Charité in November of the same year. Additionally, her capture by Burgundian troops in May 1430 during an attempt to relieve Compiègne further underscores her experiences with losses in battle.', final_answer='Yes, Joan of Arc did lose some battles, including the siege of Paris and the siege of La Charité.')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_reasoning_chain.invoke({\"question\": question, \"notebook\": notebook})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN/NH8yBU+BCOBBW8UzlzQa",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
